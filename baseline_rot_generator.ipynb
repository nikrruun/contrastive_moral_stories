{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b194b8-be8d-44e3-90dc-27f3fe4d05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ffca712-77e3-4491-8218-9cd492342c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_social_chem101():\n",
    "    a = pd.read_csv(\"data/social-chem-101/social-chem-101.v1.0.tsv\", sep=\"\\t\")\n",
    "    return a\n",
    "\n",
    "social_chem = load_social_chem101()\n",
    "#social_chem = social_chem[social_chem[\"split\"] == \"train\"]\n",
    "social_chem = social_chem.dropna(subset=[\"rot-categorization\", \"rot-judgment\", \"action\", \"rot-agree\"])\n",
    "social_chem = social_chem[social_chem[\"rot-agree\"] >= 3.0]\n",
    "social_chem = social_chem[social_chem[\"rot-bad\"] == 0]\n",
    "social_chem = social_chem[social_chem[\"rot-categorization\"].apply(lambda x: \"morality-ethics\" in x or \"social-norms\" in x)]\n",
    "social_chem = social_chem[social_chem[\"rot-judgment\"].apply(lambda x: \"{\" not in x)]\n",
    "social_chem = social_chem[social_chem.apply(lambda x: max(len(x[\"rot\"]), len(x[\"action\"]) + len(x[\"rot-judgment\"])) <= 128, axis=1)]\n",
    "social_chem = social_chem[[\"action\", \"rot-judgment\", \"rot\"]].groupby(\"rot\", as_index=False).nth(0)\n",
    "\n",
    "train, dev = train_test_split(social_chem, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7154cada-1bb8-40c2-b8c0-9fb9e79d5f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Moral Stories Github:\n",
    "# https://github.com/demelin/moral_stories/blob/master/experiments/utils.py\n",
    "\n",
    "import sacrebleu\n",
    "from sacrerouge.metrics import Rouge\n",
    "\n",
    "def compute_gen_metrics(preds, targets):\n",
    "    \"\"\" Aggregates generation metrics. \"\"\"\n",
    "    assert len(preds) == len(targets)\n",
    "    return {'BLEU-4': compute_bleu(preds, targets),\n",
    "            'ROUGE-L': compute_rouge(preds, targets)}    \n",
    "\n",
    "def compute_bleu(preds, targets):\n",
    "    \"\"\" Computes corpus-level BLEU for the generated sequences. \"\"\"\n",
    "    targets = [targets]\n",
    "    bleu = sacrebleu.corpus_bleu(preds, targets)\n",
    "    return bleu.score\n",
    "\n",
    "def compute_rouge(preds, targets):\n",
    "    \"\"\" Computes ROUGE-L for the generated sequences. \"\"\"\n",
    "    rouge = Rouge(compute_rouge_l=True)\n",
    "    rouge_out = rouge.evaluate(preds, [[tgt] for tgt in targets])\n",
    "    return rouge_out[0]['rouge-l']['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f13a10-c44f-409e-9d22-c79af220e121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU-4': 56.47984613335201, 'ROUGE-L': 89.408}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = dev[\"rot-judgment\"] + \" \" + dev[\"action\"]\n",
    "compute_gen_metrics(preds.to_list(), dev[\"rot\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91371c0-92f4-44ba-bdae-81dde5b10db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
