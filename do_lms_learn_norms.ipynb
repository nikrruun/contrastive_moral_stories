{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af18eb8-995e-4b38-8f61-20eade768e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19 23:23:31.801021: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import datasets\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd120111-b01c-4a4b-b1f0-6ab888174576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_action_norm_split(path):\n",
    "    train, dev, test = [pd.read_json(f\"{path}{x}.jsonl\", lines=True) for x in [\"train\", \"dev\", \"test\"]]\n",
    "\n",
    "    # construct dataframes that can actually be used\n",
    "    assign_action = lambda x: x[\"moral_action\"] if x[\"label\"] == 1 else x[\"immoral_action\"]\n",
    "    train[\"action\"] = train.apply(assign_action, axis=1)\n",
    "    dev[\"action\"] = dev.apply(assign_action, axis=1)\n",
    "    test[\"action\"] = test.apply(assign_action, axis=1)\n",
    "\n",
    "    subset = [\"norm\", \"action\", \"label\"]\n",
    "    train = train[subset]\n",
    "    dev = dev[subset]\n",
    "    test = test[subset]\n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "train, dev, test = load_action_norm_split(\"data/contrastive_moral_stories/original_ms/action+norm/norm_distance/\")\n",
    "opt_train, opt_dev, opt_test = load_action_norm_split(\"data/contrastive_moral_stories/optional_ms/action+norm/norm_distance/\")\n",
    "anti_train, anti_dev, anti_test = load_action_norm_split(\"data/contrastive_moral_stories/anti_ms/action+norm/norm_distance/\")\n",
    "contra_train, contra_dev, contra_test = load_action_norm_split(\"data/contrastive_moral_stories/contra_ms/action+norm/norm_distance/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0014940b-60db-44d0-8689-74fd45413b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.DatasetDict()\n",
    "\n",
    "# (N, A_M, 1)\n",
    "# normal norms, moral actions: test dataset\n",
    "# (N, A_I, 0)\n",
    "dataset[\"original_ms\"] = datasets.Dataset.from_pandas(test)\n",
    "\n",
    "# (ON, A_M, 1)\n",
    "# optional norms, normal actions: optional dataset\n",
    "# (ON, A_I, 1)\n",
    "dataset[\"optional_ms\"] = datasets.Dataset.from_pandas(opt_test)\n",
    "\n",
    "# (~N, A_M, 0)\n",
    "# (~N, A_I, 1)\n",
    "# anti_norms, negated labels\n",
    "dataset[\"anti_ms\"] =  datasets.Dataset.from_pandas(anti_test)\n",
    "\n",
    "# everything above stacked\n",
    "dataset[\"contra_ms\"] =  datasets.Dataset.from_pandas(contra_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e52c15-feab-45ea-9878-c3ca9c2a9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def tokenize(samples):\n",
    "    return tokenizer(samples[\"action\"], samples[\"norm\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "tokenized_data = dataset.map(tokenize, batched=True, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee75e2-a46b-4e49-9a06-a22e80647998",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"data/trash\",\n",
    "    logging_dir=\"data/trash\",\n",
    "    per_device_eval_batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c30206-b19a-4381-98bc-a5a7f6ccfdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file data/models/roberta-large/original_ms/bs16_lr_1e-05/checkpoint-1250/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"data/models/roberta-large/original_ms/bs16_lr_1e-05/checkpoint-1250/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file data/models/roberta-large/original_ms/bs16_lr_1e-05/checkpoint-1250/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at data/models/roberta-large/original_ms/bs16_lr_1e-05/checkpoint-1250/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 128\n",
      "loading configuration file data/models/roberta-large/optional_ms/bs16_lr_1e-05/checkpoint-1249/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"data/models/roberta-large/optional_ms/bs16_lr_1e-05/checkpoint-1249/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file data/models/roberta-large/optional_ms/bs16_lr_1e-05/checkpoint-1249/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at data/models/roberta-large/optional_ms/bs16_lr_1e-05/checkpoint-1249/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 128\n",
      "loading configuration file data/models/roberta-large/anti_ms/bs16_lr_1e-05/checkpoint-4996/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"data/models/roberta-large/anti_ms/bs16_lr_1e-05/checkpoint-4996/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file data/models/roberta-large/anti_ms/bs16_lr_1e-05/checkpoint-4996/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at data/models/roberta-large/anti_ms/bs16_lr_1e-05/checkpoint-4996/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 128\n",
      "loading configuration file data/models/roberta-large/contra_ms/bs16_lr_1e-05/checkpoint-14992/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"data/models/roberta-large/contra_ms/bs16_lr_1e-05/checkpoint-14992/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file data/models/roberta-large/contra_ms/bs16_lr_1e-05/checkpoint-14992/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at data/models/roberta-large/contra_ms/bs16_lr_1e-05/checkpoint-14992/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 128\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: action, norm. If action, norm are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6000\n",
      "  Batch size = 128\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"original_ms\":\"data/models/roberta-large/original_ms/bs16_lr_1e-05/checkpoint-1250/\",\n",
    "    \"optional_ms\":\"data/models/roberta-large/optional_ms/bs16_lr_1e-05/checkpoint-1249/\",\n",
    "    \"anti_ms\":    \"data/models/roberta-large/anti_ms/bs16_lr_1e-05/checkpoint-4996/\",\n",
    "    \"contra_ms\":  \"data/models/roberta-large/contra_ms/bs16_lr_1e-05/checkpoint-14992/\",\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(index=models.keys(), columns=tokenized_data.keys())\n",
    "\n",
    "for model_name, model_path in models.items():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    for split, data in tokenized_data.items():\n",
    "        r = trainer.evaluate(data)\n",
    "        results.loc[model_name][split] = r[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d27ed19c-9d8b-4e30-a803-23c2e2126b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_ms</th>\n",
       "      <th>optional_ms</th>\n",
       "      <th>anti_ms</th>\n",
       "      <th>contra_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>original_ms</th>\n",
       "      <td>0.9055</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.615833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optional_ms</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anti_ms</th>\n",
       "      <td>0.274</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.9105</td>\n",
       "      <td>0.568167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contra_ms</th>\n",
       "      <td>0.8875</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.8965</td>\n",
       "      <td>0.927333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            original_ms optional_ms anti_ms contra_ms\n",
       "original_ms      0.9055       0.521   0.421  0.615833\n",
       "optional_ms         0.5         1.0     0.5  0.666667\n",
       "anti_ms           0.274        0.52  0.9105  0.568167\n",
       "contra_ms        0.8875       0.998  0.8965  0.927333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b129b0-3f4f-44aa-9ae5-b720328576d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
